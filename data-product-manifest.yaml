# --- Data product definition template and documentation   --- #

# Data product manifest version. Prefer to have semantic version
version: "0.0.1"

# The meta information regarding the data product. This information will be available for searching and identifying the product within the platform
discoveryPort:
  # "id" will be generated by the meta data service in which this product is registered. It should have a unique identifier/URI in the platform
  id:
  # name might be
  name:
  description:
  # what category does the product belongs to. Eg  retail, finance. ( this wil be different from domain information)
  category:
  # for
  image:
  # The information reagardiong the user. (If the manifest is created by the platfrom userinformation is already avaliable)
  createdBy:
    name:
    email:
    userid:
    url:
  createTime:
  # The list of keywords that will help in discovery
  tags: []
  # Will be key value pairs EG: industry: "retail"
  extra: {}
  # domain information of this data product which is registered/created with in the platform
  domainInfo:
    # has to be explored more
    expandme:

# Input port is a set of channels, by which you will be receiving or pulling data. A product can have multiple input channels active at the same time with any combinations.
inputPorts:
  # The meta information of this specific input port. This information will be registared as 'sources' within the platform
  - alias:
    description:
    tags: []
    extra: {}
    # The mechanism which the data for the specific input channel is pulled from the product or pushed through the product. OPTIONS ['pull'|'push']
    syncType: 'pull'
    # each connection will have its own specification to connect to it
    connectionOptions:
      # Type of connection that the product has to establish for communication to the input data source OPTIONS ['postgres'|'mysql'|'vertica'|'s3'|'kafka'|'sftp'|'file',''....]
      type: 'jdbc'
      uri: '${env_vertica_dev_connection_uri}'
      expandme: ''
    # Entity to be pulled from the source
    entity:
      # type will depend on the connectionType, Eg for JDBC it will be table, sftp it will be file type like .csv, .parquet, kafak it will be a topic
      type:
      entityId:
      # schema as per the source of an entity. We should store the schema type as it is, supported types are parquet, avro, jsonschema
      sourceSchemaUrl:
      # expected schema after the projection is applied if there is any . Eg entity schema may be [a,b,c,d] this is the source schema
      projectSchemaUrl:
      # Filter query run on the entity , Eg country=uae & email != "mye@mail"
      filter:
      # will be column name avalable in the schema definitions, Eg country,email,user. There will not be any transformation not even colunm name
      projection:
      # the date/time the event was logged in the source system
      processingTimeColumn:
      # the business date/time of the event
      actualTimeColumn: ''
      # columns to use to create a unique key (e.g. hash of columns to be used in scd2 de-duplication)
      uniqueKeyColumns: []
      # Business columns for which to trach changes
      changeTrackColumns: []

  - alias:
    description:
    tags: []
    extra: {}
    # If the sync type is 'push' means the data product will open a communication channel and this data product can receive data via that channel
    syncType: 'push'
    # each connection will have its own specification to connect to it
    connectionOptions:
      # The channel that is required to receive the data OPTIONS ['REST','kafkaTopic','S3']
      type: REST
      path: '/${path}'
      # explored more
      expandme:
    # Entity to be pulled from the source
    entity:
      # type will depend on the connectionType, Eg for JDBC it will be table, sftp it will be file type like .csv, .parquet, kafak it will be a topic
      type:
      entityId:
      # schema as per the source of an entity. We should store the schema type as it is, supported types are parquet, avro, jsonschema
      sourceSchemaUrl:
      # expected schema after the projection is applied if there is any . Eg entity schema may be [a,b,c,d] this is the source schema
      projectSchemaUrl:
      # Filter query run on the entity , Eg country=uae & email != "mye@mail"
      filter:
      # will be column name avalable in the schema definitions, Eg country,email,user. There will not be any transformation not even colunm name
      projection:
      # the date/time the event was logged in the source system
      processingTimeColumn:
      # the business date/time of the event
      actualTimeColumn: ''
      # columns to use to create a unique key (e.g. hash of columns to be used in scd2 de-duplication)
      uniqueKeyColumns: []
      # Business columns for which to trach changes
      changeTrackColumns: []
    # an input channel can be another data product also. (The platfrom metadata layer will capture the lineage.)

  - alias:
    isDataProduct: true
    # the product identifier with in the platform
    dataProductId:
    # how data porduct will consume the linked data product
    accessOptions : {}
    # The list of entities that needs to be pulled from the source
    entities:
      # the output port id of the mentioned data product
      portId:
      # Filter query run on the entity , Eg country=uae & email != "mye@mail"
      filter:
      # will be column name avalable in the schema definitions, Eg country,email,user
      projection:
      # expected schema after the projection is applied if there is any . Eg entity schema may be [a,b,c,d] this is the source schema
      projectSchemaUrl:

# The code that will transform the data from input port and make it avalabe for output port to consume it.
transformation:
  # This can be a local or git project URL
  codeUrl:
  #
  runtime:
  language:
  libraries:

# Infromation regarding the product data and logical schema. Define the update strategy via updateStrategy options.
stateManagement:
  # the logical schema for the storage of the data product state
  logicalSchemaUrl:
  # ? histor
  retentionPeriod:
  # interval of data product has to be refreshed
  refreshInterval:
  # data update strategy (appendOnly, scd2/delta, cdc)
  updateStrategy:

  updateStrategyOptions:
    # the date/time the event was logged in the source system
    processingTimeColumn:
    # the business date/time of the event
    actualTimeColumn: ''
    # columns to use to create a unique key (e.g. hash of columns to be used in scd2 de-duplication)
    uniqueKeyColumns: []
    # Business columns for which to trach changes
    changeTrackColumns: []
    tags: []
    extra: {}

# Control the way data is consumed from the data product.
outPort:
  - alias:
    # the type/channel of the output port e.g. SQL, Rest, Cypher, Gremlin, GraphQL, CUBE-OLAP...
    queryType:
    # slo is a set of metrcis and their corresponding expected/targeted value (e.g. responseTime < 1s )
    slo: {}
    tags: []
    extra: {}

# Define the busineess rules and quality controls, SLOs etc
controlPort:
  dataQualityRules:
  businessMetrics:
  #
  slo:
  tags: []
  extra: {}

# --- end -- #

# Notes and comments

# how to choose the storage is based on how we allo quries in output port
# [text without limit] --> Vetica[var char (limit)]
# What is present in logicalSchema can be quried and get it via output port
# Eg [name text, age number] -> [name text , age text]
# refreshInterval - product will be executed based on this
